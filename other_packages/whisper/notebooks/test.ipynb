{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.audio.io.AudioFileClip import AudioFileClip\n",
    "import uuid\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = \"\"\"\n",
    "accelerate==1.81,\n",
    "moviepy==2.2.1,\n",
    "torch==2.7.1,\n",
    "torchaudio==2.7.1,\n",
    "transformers==4.53.0,\n",
    "pydantic==2.10.6,\n",
    "uvicorn==0.34.0,\n",
    "fastapi==0.115.6,\n",
    "pydantic-settings==2.9.0,\n",
    "fastapi==0.115.14,\n",
    "pydantic==2.11.7,\n",
    "uvicorn==0.35.0\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: accelerate\n",
      "Version: 1.8.1\n",
      "Location: /home/azureuser/whisper/.venv/lib/python3.10/site-packages\n",
      "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
      "Required-by:\n",
      "---\n",
      "Name: fastapi\n",
      "Version: 0.115.14\n",
      "Location: /home/azureuser/whisper/.venv/lib/python3.10/site-packages\n",
      "Requires: pydantic, starlette, typing-extensions\n",
      "Required-by:\n",
      "---\n",
      "Name: moviepy\n",
      "Version: 2.2.1\n",
      "Location: /home/azureuser/whisper/.venv/lib/python3.10/site-packages\n",
      "Requires: decorator, imageio, imageio-ffmpeg, numpy, pillow, proglog, python-dotenv\n",
      "Required-by:\n",
      "---\n",
      "Name: pydantic\n",
      "Version: 2.11.7\n",
      "Location: /home/azureuser/whisper/.venv/lib/python3.10/site-packages\n",
      "Requires: annotated-types, pydantic-core, typing-extensions, typing-inspection\n",
      "Required-by: fastapi\n",
      "---\n",
      "Name: torch\n",
      "Version: 2.7.1\n",
      "Location: /home/azureuser/whisper/.venv/lib/python3.10/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-cufile-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-cusparselt-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvtx-cu12, sympy, triton, typing-extensions\n",
      "Required-by: accelerate, torchaudio\n",
      "---\n",
      "Name: torchaudio\n",
      "Version: 2.7.1\n",
      "Location: /home/azureuser/whisper/.venv/lib/python3.10/site-packages\n",
      "Requires: torch\n",
      "Required-by:\n",
      "---\n",
      "Name: transformers\n",
      "Version: 4.53.0\n",
      "Location: /home/azureuser/whisper/.venv/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by:\n",
      "---\n",
      "Name: uvicorn\n",
      "Version: 0.35.0\n",
      "Location: /home/azureuser/whisper/.venv/lib/python3.10/site-packages\n",
      "Requires: click, h11, typing-extensions\n",
      "Required-by:\n"
     ]
    }
   ],
   "source": [
    "!uv pip show torch torchaudio transformers accelerate moviepy pydantic uvicorn fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in test_audio-e011ebfb-4bd7-4735-bec5-338a17ab1c12.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:   0%|                                                             | 0/706 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "audio = AudioFileClip(\"test_audio.mp4\")\n",
    "out_file = f\"test_audio-{uuid.uuid4()}.mp3\"\n",
    "audio.write_audiofile(out_file)\n",
    "with open(out_file, \"rb\") as f:\n",
    "    audio_bts = f.read()\n",
    "Path(out_file).unlink()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    return_timestamps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process bytes\n",
    "def bts_to_np(audio_bytes: bytes):\n",
    "    # Convert bytes to waveform using torchaudio\n",
    "    with io.BytesIO(audio_bytes) as audio_file:\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "    # Whisper expects 16kHz audio\n",
    "    if sample_rate != 16000:\n",
    "        waveform = torchaudio.functional.resample(waveform, orig_freq=sample_rate, new_freq=16000)\n",
    "\n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "    # Convert waveform to numpy array\n",
    "    waveform_np = waveform.squeeze().numpy()\n",
    "    return waveform_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_np = bts_to_np(audio_bts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/whisper/.venv/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:604: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "result = pipe(waveform_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Then we need to create a ScreenPipe MCP server so we can query the transcription. And we need to try out the GitHub MCP server for creating tickets. And lastly, we need to try to run the end-to-end flow. Thank you.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/whisper/.venv/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:604: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "result = pipe(\"test_audio.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Then we need to create a ScreenPipe MCP server so we can query the transcription. And we need to try out the GitHub MCP server for creating tickets. And lastly, we need to try to run the end-to-end flow. Thank you.', 'chunks': [{'timestamp': (0.0, 10.0), 'text': ' Then we need to create a ScreenPipe MCP server so we can query the transcription.'}, {'timestamp': (10.0, 18.0), 'text': ' And we need to try out the GitHub MCP server for creating tickets.'}, {'timestamp': (18.0, 23.0), 'text': ' And lastly, we need to try to run the end-to-end flow.'}, {'timestamp': (30.0, 59.98), 'text': ' Thank you.'}]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Then we need to create a ScreenPipe MCP server so we can query the transcription. And we need to try out the GitHub MCP server for creating tickets. And lastly, we need to try to run the end-to-end flow. Thank you.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = result[\"text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Then we need to create a ScreenPipe MCP server so we can query the transcription. And we need to try out the GitHub MCP server for creating tickets. And lastly, we need to try to run the end-to-end flow. Thank you.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
